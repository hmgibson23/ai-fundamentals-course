{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ce1a46-b899-4302-b61a-efa9cfa252ba",
   "metadata": {},
   "source": [
    "# Fine-tune a Transformer\n",
    "\n",
    "Using Huggingface and Pytorch we can very easily take what we've learnt here and fine-tune a transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a99a139-3098-4966-8b1c-c9691b9783dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# this is the model we're going to use\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# set up the model with the number of labels in the dataset\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5, ignore_mismatched_sizes=True)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f13abbb8-8959-4895-8643-cca332d18300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>dr. goldberg offers everything i look for in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Got a letter in the mail last week that said D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      4  dr. goldberg offers everything i look for in a...\n",
       "1      1  Unfortunately, the frustration of being Dr. Go...\n",
       "2      3  Been going to Dr. Goldberg for over 10 years. ...\n",
       "3      3  Got a letter in the mail last week that said D...\n",
       "4      0  I don't know what Dr. Goldberg was like before..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's have a look at the dataset\n",
    "dataset.set_format(type='pandas')\n",
    "df = dataset['train'][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cb78e6b-18ad-4f79-9936-00191ac34eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2852,  1012, 18522,  4107,  2673,  1045,  2298,  2005,  1999,\n",
       "          1037,  2236, 18742,  1012,  2002,  1005,  1055,  3835,  1998,  3733,\n",
       "          2000,  2831,  2000,  2302,  2108,  9161,  6026,  1025,  2002,  1005,\n",
       "          1055,  2467,  2006,  2051,  1999,  3773,  2010,  5022,  1025,  2002,\n",
       "          1005,  1055,  6989,  2007,  1037,  2327,  1011, 18624,  2902,  1006,\n",
       "         27935,  1007,  2029,  2026,  3008,  2031,  4541,  2000,  2033,  2003,\n",
       "          2200,  2590,  1999,  2553,  2242,  6433,  1998,  2017,  2342,  5970,\n",
       "          1025,  1998,  2017,  2064,  2131,  6523,  7941,  2015,  2000,  2156,\n",
       "         15744,  2302,  2383,  2000,  2156,  2032,  2034,  1012,  2428,  1010,\n",
       "          2054,  2062,  2079,  2017,  2342,  1029,  1045,  1005,  1049,  3564,\n",
       "          2182,  2667,  2000,  2228,  1997,  2151, 10821,  1045,  2031,  2055,\n",
       "          2032,  1010,  2021,  1045,  1005,  1049,  2428,  5059,  1037,  8744,\n",
       "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's tokenize one item to see what it looks like\n",
    "one = tokenizer(df['text'][0], max_length=512, padding=\"max_length\", return_tensors=\"pt\", truncation=True)\n",
    "one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f49c5f0-6f74-4054-a083-dae086219eee",
   "metadata": {},
   "source": [
    "The input_ids are the tokens generated from the text and the attention_mask is telling the transformer which tokens to pay attention to. Don't worry too much about this. It's just the context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b12e5287-582f-4c30-9c3b-5c3c724ac8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one['input_ids'].shape\n",
    "one['input_ids'].squeeze(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa671d30-b2ec-431b-b3e3-33dd15090c34",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c3504a4-93b3-40d1-9cae-fbbdc7cce0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's put this in a torch dataset \n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class TrainingSet(Dataset):\n",
    "    def __init__(self, text, labels, tokenizer=None):\n",
    "        self.texts = text\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # here we generate the encodings\n",
    "        encoding = self.tokenizer(self.texts[idx], max_length=512, padding=\"max_length\", return_tensors=\"pt\", truncation=True)\n",
    "        # which will give us a tensor shap (1,512) - because of return_tensors=\"pt\" i.e pytorch tensors\n",
    "        # we actually want a shape (512) so we apply squeeze (which reshapes the tensor at 0), which removes the dimension\n",
    "        # let's also move the data to the gpu each time we get one\n",
    "        input_ids = encoding['input_ids'].squeeze(0).to(device)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0).to(device)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ffcaeca-4840-4cc7-93f4-485204421322",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TrainingSet(df['text'], df['label'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e8dd58e-e92c-4e7f-8ddc-42194ba5106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  2852,  1012, 18522,  4107,  2673,  1045,  2298,  2005,  1999,\n",
       "          1037,  2236, 18742,  1012,  2002,  1005,  1055,  3835,  1998,  3733,\n",
       "          2000,  2831,  2000,  2302,  2108,  9161,  6026,  1025,  2002,  1005,\n",
       "          1055,  2467,  2006,  2051,  1999,  3773,  2010,  5022,  1025,  2002,\n",
       "          1005,  1055,  6989,  2007,  1037,  2327,  1011, 18624,  2902,  1006,\n",
       "         27935,  1007,  2029,  2026,  3008,  2031,  4541,  2000,  2033,  2003,\n",
       "          2200,  2590,  1999,  2553,  2242,  6433,  1998,  2017,  2342,  5970,\n",
       "          1025,  1998,  2017,  2064,  2131,  6523,  7941,  2015,  2000,  2156,\n",
       "         15744,  2302,  2383,  2000,  2156,  2032,  2034,  1012,  2428,  1010,\n",
       "          2054,  2062,  2079,  2017,  2342,  1029,  1045,  1005,  1049,  3564,\n",
       "          2182,  2667,  2000,  2228,  1997,  2151, 10821,  1045,  2031,  2055,\n",
       "          2032,  1010,  2021,  1045,  1005,  1049,  2428,  5059,  1037,  8744,\n",
       "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0], device='mps:0'),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0], device='mps:0'),\n",
       " 'labels': 4}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can just get any item in our training set\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d4619e2-eb45-4157-aa40-00cb4816f570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but we want to do batch loading so\n",
    "# a batch is the number of samples to pass through the model BEFORE its weights get updated. \n",
    "# This is important because it constraints memory to just the batch size. It will also allow the gpu to run in parallel, \n",
    "# which obviously gpus are good at\n",
    "\n",
    "# to achieve this we use a dataloader\n",
    "train_loader = DataLoader(train, batch_size=8)\n",
    "next(iter(train_loader))['input_ids'].shape (8,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88604bbb-7bc2-4966-a79f-9376cda2e1ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the model being called on a batch \n",
    "batch = next(iter(train_loader))\n",
    "output = model(batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "output.logits.shape # (8, 5) - 8 inference results with 5 probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79aaff0d-cac9-4d29-9267-47a6876d4e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2952, 0.1827, 0.1856, 0.1433, 0.1932],\n",
       "        [0.3687, 0.1883, 0.1639, 0.1263, 0.1528],\n",
       "        [0.1975, 0.1857, 0.1205, 0.1459, 0.3505],\n",
       "        [0.3693, 0.1968, 0.1571, 0.1313, 0.1455],\n",
       "        [0.3579, 0.1892, 0.1626, 0.1265, 0.1637],\n",
       "        [0.1958, 0.1911, 0.1153, 0.1475, 0.3505],\n",
       "        [0.1955, 0.1898, 0.1075, 0.1477, 0.3595],\n",
       "        [0.3555, 0.1825, 0.1674, 0.1325, 0.1622]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.softmax(1) # convert to actual probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c34aaa5-514a-4ac8-bf0e-5633786c022c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4253, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's make a loss function \n",
    "# since we're doing classification we'll use CrossEntropyLoss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# test it with a batch \n",
    "test = output.logits\n",
    "out = batch['labels']\n",
    "loss = loss_fn(test, out)\n",
    "loss # so this is our loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f600051-f378-499c-9cd1-d7e66f5eaa49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's have a quick look at the model and move it to the gpu as well\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3c801fb-10d8-45f2-a651-fa791e7120d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that it's final layer (classifier) has a dimension 768 -> 5 i.e. 5 possible labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f56d42d7-d7e5-4aba-82e6-9e18c65ce927",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() # can only do this once - one round of backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "159b8b0b-a38e-414f-8ea8-4b757b28eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now make an optimize and define a training loop \n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38999ffb-0b4c-4c35-8b44-86b270962c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 1.4253073930740356\n",
      "Epoch 2 - Loss: 22801625088.0\n",
      "Epoch 3 - Loss: 356660150272.0\n",
      "Epoch 4 - Loss: 95742312448.0\n",
      "Epoch 5 - Loss: nan\n"
     ]
    }
   ],
   "source": [
    "# training loop \n",
    "epochs = 5 \n",
    "\n",
    "# init the loss function \n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 1. zero the optimizer gradients\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # 2. get a batch \n",
    "    batch = next(iter(train_loader))\n",
    "\n",
    "    # 3. get some outputs (and the labels)\n",
    "    outputs = model(batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "    labels = batch['labels']\n",
    "\n",
    "    # 4. Calculate the loss and do back propagation\n",
    "    loss = loss_fn(outputs.logits, labels)\n",
    "    loss.backward() \n",
    "    \n",
    "    # 5. Step the optimizer\n",
    "    optim.step()\n",
    "\n",
    "    # 6. Do it all again \n",
    "    print(f\"Epoch {epoch+1} - Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29030bc-d9ce-4d36-87a0-12979f279387",
   "metadata": {},
   "source": [
    "And there we go. We've fine-tuned a huggingface model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c26d72f-2614-4bb3-8356-c280cf7441d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
